# delivery-nutanix-iac

This repository is a collection of IaC modules for Nutanix. It currently contains a module for creating subnets and a module for creating RKE2 clusters. For details about using each module, see the README for each in the module subdirectories.

## RKE2 Module

This module depends on a Nutanix cluster already being configured with a subnet as well as an image being available that was built by the [uds-rke2-image-builder](https://github.com/defenseunicorns/uds-rke2-image-builder). Images built by that repo contain all needed RKE2 dependencies and the rke2 startup script that this module expects to be available that handles configuring, starting, and joining RKE2 nodes.

An example set of variables passed to the module looks something like this:
```
nutanix_cluster     = "Your-Cluster"
nutanix_subnet      = "Your Subnet"
name                = "example-rke2"
server_count        = 3
agent_count         = 3
server_memory       = 16 * 1024
server_cpu          = 8
agent_memory        = 32 * 1024
agent_cpu           = 16
image_name          = "uds-rke2-rhel-some-build-tag"
ssh_authorized_keys = ["ssh-rsa rest of your key here", "ssh-rsa different key here"]
server_dns_name     = "example-rke2.your-domain.com"
```

This set of variables would configure the module to deploy 6 VMs into the subnet named "Your Subnet" in the Nutanix cluster named "Your-Cluster". 3 of the VMs would be configured as RKE2 control plane nodes with 8 CPU and 16GiB of memory, and the other 3 VMs would be configured as agent nodes with 16 CPU and 32GiB of memory. The VMs would all be given names prefixed with "example-rke2" followed by a unique identifier and a count. For example the control plane VM names might look like "example-rke2-abc-server-0", "example-rke2-abc-server-1", etc. This would also configure each VM to allow signing in with either of the SSH keys added to the ssh_authorized_keys list. The current cloud-config configures all VMs with a default user named `nutanix` that can only log in with one of the SSH keys provided.

The `server_dns_name` variable configures both the hostname used by nodes to join the cluster, as well as the hostname you expect to use to access the kube-api. This hostname is added as a TLS SAN to the cert used by kube-api which is required for kubectl to trust the certificate when hitting the API with a hostname. When setting this variable, it is required that DNS be updated to route the hostname either to a TCP load balancer or directly to the server node IPs once the VMs are created and acquire IPs. This enables nodes to join the cluster eventually as the bootstrap and subsequent server nodes startup and join. If this variable is not set then nodes will join the cluster using the IP address of the bootstrap control plane node (the first one that starts up) and the API certs will only be valid for the control plane private IPs and the loopback address. It is not recommended to deploy a cluster without setting this to a domain name that you can configure to route to the control plane nodes.

After the cluster nodes are deployed, connect to any of the server nodes and wait for cloud-init to finish. Once it is done you can find the default admin kubeconfig file at `/etc/rancher/rke2/rke2.yaml` and kubectl at `/var/lib/rancher/rke2/bin/kubectl`. The server-0 node will be the first node to come up since it is the bootstrap node, so that is usually a good choice to connect to while waiting on the cluster to finish deploying.

## Postgres Profile VM module

This module stands up a VM that is preconfigured with all the settings required for the NDB service to import it for the purpose of creating a new Postgres database profile. It requires a VM image that has been built by the delivery-nutanix-image-builder repo that contains all the prerequisite software installed. An example set of variables passed to the module looks something like this:

```
nutanix_cluster     = "Your-Cluster"
nutanix_subnet      = "Your Subnet"
image_name          = "uds-postgresql-buildtag"
ssh_authorized_keys = ["Your SSH public key"]
user_password       = "$6$eqCKlTML5rIALmwB$6fyOlrTK9E353ofDISHuEJxqIZx8MYt.mQM.qfbeydQX/CGnz204AlmYg5VCZ8O/xLKJ34CkgV7hyoUno08N9."
pg_password         = "desired-postgres-user-password"
```

The image name should be an image available in the cluster that was built by the postgres-profile image builder. user_password is the password that will be configured for the 'era' user which is used by NDB to connect to the instance. The hashed value can be generated by using the command `mkpasswd -m sha-512 -s` or `openssl passwd -6` and providing the desired password. pg_password is the password that will be set for the postgres user and is also used by the NDB service to import the database. The passwords can be anything and will not persist when NDB is used to provision new databases in the future since NDB lets you set passwords at database provision time.

When the postgres image being used has SELinux enabled (if using RHEL this is the default), the following is a required `Post Create Command` when provisioning a database with the profile:

```
sudo touch /.autorelabel
```

On the next reboot, this fixes SELinux labels that prevent the era_postgres service from running correctly and failing to start on future reboots. This is taken care of automatically when using the ndb-pg-db module in this repo to provision NDB databases.

## NDB PG DB module

This module is used to provision postgres databases using the NDB service. A prerequisite is having the NDB service installed and configured in Nutanix and having any profiles you reference must already be created. An example set of variables passed to the module looks something like this:

```
nutanix_cluster_name   = "Your-Nutanix-Cluster"
ssh_authorized_key     = "your ssh public key"
db_password            = "some-database-password"
vm_password            = "some-vm-user-password"
software_profile_name  = "postgres_14.9"
compute_profile_name   = "large-compute"
network_profile_name   = "DEFAULT_OOB_POSTGRESQL_NETWORK"
db_param_profile_name  = "DEFAULT_POSTGRES_PARAMS"
sla_name               = "DEFAULT_OOB_BRASS_SLA"
database_name          = "yourservicedb"
instance_name          = "vm-name"
database_size          = "200" # initial database size in GiB
```

There are some limitations to managing DB VMs using the NDB service (and by extension, this module). Once a DB VM is created by the NDB service it is not possible to update the VM settings using the NDB service. For example, the compute profile chosen determines the CPU and memory allocation at deploy time, but once the VM is created if these resources need updated it would need to be done via Prism. Once a database is created the only things that can be updated through the NDB service are database size (so if a database needs more storage it can be expanded via NDB), time machine settings, and what databases exist in a postgres instance (you can add and delete databases from the NDB console). This unfortunately means this module is great for provisioning postgres databases, but not very good for managing them after they exist.

When deleting databases provisioned with this module using terraform (either by removing a module definition in a deployment or using terraform destroy) NDB only deletes the NDB Time Machine and removes the database from the database service. It does not delete the VM or the underlying storage volumes. After deleting a database with terraform, to finish cleaning up resources use the "Database Server VMs" view in the NDB console. This page will let you select the VMs that are still registered but aren't being used by NDB anymore and remove them. To do this select a VM that is no longer in use, choose the remove action, and then make sure the "Delete the VM and associated storage" box is checked so the NDB service will correctly finish cleaning up.