# Deploying Infrastructure for UDS SWF

## Overview

The UDS SWF requires a Kubernetes cluster, several external postgres databases, and external object storage.

For Kubernetes, we are using RKE2 deployed using the [rke2-module](./rke2-module.md) which depends on an image being imported into the Nutanix image store that was built by the [uds-rke2-image-builder](https://github.com/defenseunicorns/uds-rke2-image-builder). This repo uses packer to generate an image containing all of the RKE2 dependencies, a helper script used to start RKE2 at deploy time, as well as a variety of security settings preconfigured.

For postgres databases, we are using the Nutanix Database Service to deploy and manage database VMs. Nutanix Database Service is a dependency of the [ndb-pg-db](./ndb-pg-db-module.md) and [postgres-profile-vm](./postgres-profile-module.md) modules, and it will need to be installed before following this guide.

For external object storage, we assume that Nutanix Objects is installed and available. Unfortunately, the Nutanix terraform provider doesn't support managing Nutanix Objects buckets, so those will need to be created manually. The full list of buckets that need to be created are documented in the UDS SFW Bundle documentation and won't be covered here.

# Steps

## RKE2 cluster deployment

* Import latest RKE2 image into Nutanix Images
* Use the [RKE2 example deployment](../examples/rke2-cluster/) and [rke2-module docs](./rke2-module.md) as a reference to create an RKE2 deployment for your environment
* Configure DNS entries for the hostname passed into the `server_dns_name` variable. The hostname should either resolve directly to the server nodes or some kind of proxy that then routes traffic to the server nodes
* The server node IPs can either be assigned by DHCP and retreived after the server node VMs are created by Nutanix, or they can be assigned statically using the `server_ip_list` variable. A benefit of setting them statically is being able to configure DNS or proxy routing before VMs are created since the IPs are already known.
* Once `terraform apply` finishes and the RKE2 VMs are created, SSH onto the `server-0` VM and run `cloud-init status --wait` to wait for cloud init to finish running
* Once cloud-init has finished, you can create the `.kube` directory in your current users home directory, and run `sudo cp /etc/rancher/rke2/rke2.yaml ~/.kube/config && sudo chown $USER:$USER ~/.kube/config`  to copy the default root kubeconfig to your home directory
* You can then run `sudo cp /var/lib/rancher/rke2/bin/kubectl /usr/local/bin && sudo chmod +x /usr/local/bin/kubectl` to make the kubectl command available on your path
* Run `watch kubectl get nodes` to watch for the rest of the nodes to join the cluster
* Once you see all the server and agent nodes join and enter a ready state, the cluster has finished provisioning and is ready to use. Copy the `~/.kube/config file` contents somewhere that it can be referenced later for zarf to use, but update the server line from `server: https://0.0.0.0:6443` to use the DNS name you configured for your kubeAPI like `server: https://rke2.example.bigbang.dev:6443`. Note that this file is sensitive since it gives full cluster admin permissions to anything in the cluster.

## Updated Postgres Profile

Out of the box, the NDB service only has a postgres 10 profile available. Currently the latest it supports importing is postgres 14. For a new environment, a new postgres profile will need to be created before deploying postgres databases for the SWF.

* Import latest postgres profile image generated by the [delivery-nutanix-image-builder](https://github.com/defenseunicorns/delivery-nutanix-image-builder) into Nutanix Images. This image contains an updated version of postgres and all of the dependencies needed by NDB to import a VM deployed with the image
* Use the [postgres profile example deployment](../examples/postgres-profile-vm/) and [ndb-pg-db docs](./postgres-profile-module.md) as a reference to deploy a profile VM
* After the VM is up, import it into NDB
* After importing the VM, create a new postgres profile from the imported VM
* After a profile has been created from the VM, the VM can be deleted

## NDB Service Provisioned Databases

* Before provisioning any databases with the NDB service create any compute profiles you want to be available for database VMs. Out of the box there is only 1 or 2 compute profiles available, which doesn't give much flexibility when sizing postgres VM resources
* Use the [NDB postgres example deployment](../examples/swf-postgres-dbs/) and [ndb-pg-db docs](./ndb-pg-db-module.md) as a reference to create a postgres database deployment for your environment
* The example reference shows all the databases currently required for the full UDS SWF deployment. The `database_name` for each module declaration is the default database name expected by each service, so if you don't change these values then you won't need to customize the database names when deploying the UDS bundle later. It is recommended to leave them as the defaults for simplicity. The `instance_name` can be customized however you like and only changes how the VM gets named.
* The `terraform apply` for this will take a while to finish since the NDB service creates databases sequentially instead of in parallel. If more DBs are needed in the future, we recommend not trying to deploy more than the 7 that are in the example deployment at once via terraform since terraform will timeout before they finish.
* Keep track of the database and vm passwords you pass to each of these. The database passwords will be needed for services to connect to the databases using the postgres user, and the vm password would be needed to connect to the database VM as the `era` user if you somehow lose or don't set an SSH key at provision time. Ideally these will all use unique passwords.
* Once all of the postgres databases have finished provisioning, in order for the UDS SWF capabilities to utilize them DNS entries will need to be configured for each of these VMs
* Once you have created all 7 DNS entries, the postgres databases are ready to use by the SWF bundle.

## Infrastructure Deployed

At this point all of the IaC needed for the UDS SWF to be deployed into Nutanix has been deployed. UDS SWF deployment instructions can now be followed to create the necessary Objects buckets and deploy the bundle into the cluster that was just deployed.